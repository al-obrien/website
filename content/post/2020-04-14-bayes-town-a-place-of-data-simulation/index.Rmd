---
title: 'Bayes'' Town: A place of data simulation'
author: al-obrien
date: '2020-09-24'
slug: bayes-town-a-place-of-data-simulation
categories: []
tags:
  - R
  - bayesian
  - simulation
subtitle: ''
summary: ''
authors: [al-obrien]
lastmod: '2020-09-26T10:27:06-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: yes
projects: []
draft: false
diagram: true
codefolding_show: show
---

```{r setup, include = FALSE, message = FALSE, error = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE, error = FALSE, warning = FALSE, echo = TRUE)
set.seed(11)
```

```{r style, include = FALSE}
theme_eD <- function() {
  theme_minimal() %+replace%
  theme(
    panel.grid.major.x = element_blank(),
    axis.title = element_text(face = 'bold'),
    strip.text = element_text(face = 'bold'),
    legend.position = 'top',
    legend.title = element_blank(),
    legend.key.size = unit(2, 'line')
  )
}
```

```{r simluation_code, include = FALSE}

# ------------------ #
# Required libraries
# ------------------ #
library(R6)
library(rjags)
library(dplyr)
library(magrittr)

# ------------------ #
# Create class for simulation

# Alternatives would be S3 or S4, which
# have their own merits too, but many users
# will find R6 more intuitive if coming from other OOP 
# languages.
# ------------------ #

# Create class, using UpperCamelCase convention
BayesTown <- R6Class('BayesTown', list(
  
  # Public variables
  population_size = NA,
  jags_model = NULL,
  jags_parameters = NA,
  model_location = NULL,
  n.iter = NULL,
  data = NULL,
  mcmclist = NULL,
  varnames = NULL,
  
  
  # Public methods
  # Create samples from this dataset
  resample = function(...){
    
   # temp <- if(exists(n.chains)) {round(self$population_size/n.chains, 0)} else {self$population_size}
    
    cat('\n----------------- SAMPLING ', self$population_size, ' times -----------------\n')
    data = rjags::coda.samples(model = self$jags_model,
                               variable.names = self$varnames, 
                               n.iter = round(self$population_size/self$jags_model$nchain(), 0))
    dataOut = purrr::map_df(data, ~rbind(as.data.frame(.)))
    self$data <- dataOut
    self$mcmclist <- data
    
    # Print sample of the data
    cat('Data sample: \n')
    print.data.frame(head(self$data))
    
    invisible(self)
  },
  
  # Allow to have jags_model to be a string or a file
  # Let sampling occur right away
  initialize = function(population_size, jags_model, jags_data, n.iter = 5000, ...) {
    
    # Check conditions
    stopifnot(file.exists(jags_model)|is.character(jags_model))
    stopifnot(is.numeric(population_size), length(population_size) ==1)
    
    if(is.character(jags_model)){
      
      # Make tempfile for model
      temp_loc <- tempfile()
      sink(temp_loc)
      cat(jags_model)
      sink()
      
    } else if (file.exists(jags_model)) {
      
      temp_loc <- file.path(jags_model)
      
    }
    
    # Assign the self values
    self$n.iter <- n.iter
    self$population_size <- population_size
    self$jags_model <- rjags::jags.model(temp_loc, jags_data, ...)
    self$varnames <- variable.names(self$jags_model)
    
    cat('\n-------------- BURNING IN ', self$n.iter, ' iterations --------------\n')
    rjags:::update.jags(object = self$jags_model, n.iter = self$n.iter)
    self$model_location <- temp_loc
    
    # Data sample (this is first one, so not really a sample)
    self$resample()
    
  },
  
  # Method to output key information about the sample
  print = function(...){
    cat('-------------------- MODEL OVERVIEW --------------------\n')
    cat('Model Location: ', self$model_location, '\n')
    cat('MCMC burn-in iterations: ', self$n.iter, '\n')
    cat('JAGS Model: ', self$jags_model$model(), sep = '\n')
    cat('\n-------------------- SAMPLE DATA --------------------\n')
    cat('Population Size: ', self$population_size, '\n')
    cat('Sample data: \n')
    print.data.frame(head(self$data, 10))
    invisible(self)
  },
  
  # Method to extract JAGS model (needs to be in 'mcmc' form to work with coda)
  summary = function(...){
    cat('--------------- SUMMARY DATA ---------------')
    summary(self$mcmclist)
  },
  
  # Set variables (will help to do re-sampling easier)
  set_variables = function(varnames = variable.names(self$jags_model), mode = 'include'){
    stopifnot(is.character(varnames), all(varnames %in% variable.names(self$jags_model)))
    stopifnot(mode %in% c('exclude', 'include'))
    
    if(mode == 'exclude'){
      allvars <- variable.names(self$jags_model)
      keep_only <- allvars[!(allvars %in% varnames)]
      cat(keep_only)
      self$varnames <- keep_only
      cat('Variables of interest set to: ', paste(self$varnames, collapse = ', '))
      
    } else if (mode == 'include'){
      self$varnames <- varnames
      cat('Variables of interest set to: ', paste(self$varnames, collapse = ', '))
    }
    
    invisible(self)
  },
  
  # Plot diagnostics (convenience wrapper around coda)
  plot = function(type = 'trace', ...){
    stopifnot(type %in% c('trace', 'gelman', 'geweke', 'autocorr'))
    
    for(i in unique(type)){
      switch(i,
             'trace' = coda::traceplot(self$mcmclist, ...),
             'gelman' = coda::gelman.plot(self$mcmclist, ...),
             'geweke' =  coda::geweke.plot(self$mcmclist, ...),
             'autocorr' = coda::autocorr.plot(self$mcmclist, ...))
    }
  }
))

```


## Welcome to Bayes' Town

Bayes' Town is a special place, a place where we can be omnipotent and omniscient. We take comfort in having knowledge and control of all things, even though we know the entire place is apart from reality, a muddled reflection at best and a complete fantasy at worst. Bayes' Town is a simulation, a useful tool we can use to explore scenarios and test hypotheses, based in reality or otherwise.

For public health, creating a synthetic population can be a valuable tool. Population characteristics, and relations between them, can be modeled based upon prior knowledge or to understand 'what-if' scenarios. Sometimes, it even is just a good way to better understand how a model is working. If you know the generative process of a synthetic population, then you can see how well certain models perform under those circumstances. More on this later...


## Simulating Bayes' Town's Blight

As we are the masters of our simulated domain, we can decide exactly how the characteristics of our Bayes' Town citizens are connected. Citizens of Bayes' Town are very concerned about disease, specifically the rampant *Disease X*. Fortunately for us, as the creators we can instantly summon the image of how the population is impacted by this pestilence. We can visualize the relationship as an network between variables of *sex*, *age*, and whether or not the individual lives in an *urban or rural* location. Using [`DiagrammeR`](https://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html) and the `DOT` language, we can visualize this relationship.

```{r echo = FALSE, collapse = T, fig.cap = 'The "known" relationship between Disease X and the citizens of Bayes\' Town.', out.height = '100%', out.width = "100%"}
library(DiagrammeR)

DiagrammeR::grViz("digraph {
  
  # Set graph
  graph [layout = dot, rankdir = TB]
  
  # Nodes overall
  node [shape = oval, style = filled, fontcolor = '#394045']  
  
  # Explanatory nodes
  node [fillcolor = '#5cb8ff', color = '#3f82b5']  
  sex [label = 'Sex']
  age [label = 'Age']
  urb [label =  'Urban/Rural']
  
  # Outcome node
  node [fillcolor = '#ffc15e', color = '#c99747']
  disease [label = 'Disease X']
  
  # Edges
  edge [color = '#848c91']
  sex -> age
  {sex urb age} -> disease
  }")
```

### Generate Bayes' Town 
With this knowledge, we can generate data on our citizens for this particular scenario. The core functionality is powered by `rJAGS` but to make it a bit easier to use, the process is wrapped up into an `R6` object called `BayesTown`. This should be familiar to anyone from object-orientated programming (OOP) languages like Python but perhaps a bit foreign for those more comfortable with multiple dispatch, which is the common approach in R (`S3` and `S4`) and Julia. To create a population of 1,000 people we provide some predefined information to the `new` (construct) method. The `inputParam` and `simModel` values will be discussed later in the [methodology section](#data-simulation-with-bayesian-networks).

```{r simulation_input, include = FALSE}
# ------------------ #
# Initially used to create the dataset
# ------------------ #

# Set input parameters
inputParam <- list(p_sex = 0.52, 
                   age_shape = 6,
                   age_scale = 6,
                   age_param = 0.7)

# Define model
simModel <-  as.character('
                      model {
                        a_disease ~ dnorm(-10, 10);
                        b_age ~ dnorm(.015, 10);
                        b_sex ~ dnorm(.005, 10);
                        b_urbanity ~ dnorm(5, 10);
                      
                        sex ~ dbinom(p_sex, 1);
                        urbanRural ~ dbinom(0.8, 1);
                        age ~ dgamma((age_shape + age_param * sex), 1/age_scale);
                      
                        logit(p) <- a_disease + b_age * age + b_sex * sex + b_urbanity * urbanRural;
                        disease ~ dbinom(p, 1);
                      }')
```

```{r, results = FALSE, eval =  FALSE}
newTown <- BayesTown$new(population_size = 1000,
                      jags_data = inputParam, 
                      n.chains = 2,
                      n.iter = 5000,
                      jags_model = simModel)
```

```{r loadData, include = F}
# ------------------ #
# Load saved file to keep consistent
# ------------------ #

newTown <- readRDS('./bayesTownSimulation.RDS')
```


The output provides some basic information on the model graph and a snapshot of the data sample. This includes all the information from the simulation, including the various data variables, coefficients, and distribution parameters. Due to sampling variation, subsequent runs with the same parameters will create slightly different populations. 

### Explore simulated population
To explore the simulated data, we extract the variables we observed in the [diagram above](#htmlwidget-1). This is easily done by [method chaining](https://en.wikipedia.org/wiki/Method_chaining) `set_variables` (to sample only particular variables/parameters) and `resample` (to rerun the simulation).

```{r, eval = F}
# Import library for some exploration work
library(dplyr)
library(magrittr)

# Data sample just to include the set of variables in the list
newTown$set_variables(c('age', 'sex', 'urbanRural', 'disease'), mode = 'include')$resample()

# Output first 5 rows of simulated data
head(newTown$data, 5)

# Summarize some basic information using dplyr
newTown$data %>% 
  group_by(sex, urbanRural) %>%
  summarise(meanAge = mean(age),
            diseasePerc = 100 * (sum(disease)/nrow(.))) 
```

Here is a sample of the simulated data:
```{r echo = FALSE}
head(newTown$data, 10)
```

And here are some summary statistics:
```{r echo = FALSE}
newTown$data %>% group_by(sex, urbanRural) %>% summarise(meanAge = mean(age), diseasePerc = 100 * (sum(disease)/nrow(.))) 
```

Just from this basic summary, we can see that **(a)** urban areas have a larger burden of disease and **(b)** females and older age groups on average have a slightly higher proportion of disease. Tables are nice, but pictures are better. Let's create a few plots to see how the characteristics of Bayes' Town citizens relate to disease status.

Age among females is slightly higher on average than their male counterparts.
```{r, echo = FALSE, collapse = T, fig.cap = 'Distribution of disease by age and sex.', out.width = "100%", out.extra = "style= 'margin-bottom: 0rem;'"}
# Load plotting library
library(ggplot2)

# Minor adjustment to data for plotting
plotData <- newTown$data %>% 
  mutate(sex = if_else(sex == 1, 'Female', 'Male'),
         age = floor(age))

# Age-Sex distribution
ggplot(plotData) +
  geom_boxplot(aes(x = sex, y = age, fill = sex),
               outlier.fill = NULL, 
               outlier.shape = 21, 
               width = 0.5) +
  xlab('Sex') +
  ylab('Age') +
  theme_eD() + # Custom theme
  scale_fill_manual(values = c('Male' = '#5cb8ff',
                               'Female' = '#ff8fb7'))
```

We could plot each variable against the outcome (disease) separately but since the data has just 4 variables it is possible to observe the relationships at the same time. This may not be always preferable, as the visualization can become quite **dense**; that is to say, it could be hard to digest all of the dimensions presented from the data. How the variables are assigned to plot are important for aiding interpretation. As our interest is the presence/absence of *Disease X*, we assign this variable as the plot color to ensure it is included in each panel. In this instance plotting all the variables does provide some interesting insights:

  - Males appear to have a larger representation in the younger age groups  
  - Disease is present in higher proportions among older age groups  

```{r, echo = FALSE, fig.cap = '<i>Disease X</i> in terms of age, sex, and location.', out.height = '100%', out.width = "100%", out.extra = "style= 'margin-bottom: 0rem;'" }
# Disease by age, sex, urban
plotData %>%
  mutate(urbanRural = factor(urbanRural, labels = c('Rural', 'Urban')),
         disease = factor(disease, labels = c('No Disease', 'Disease X'))) %>%
  
  ggplot(aes(x = age)) +
  geom_density(aes(fill = disease), color = 'transparent', alpha = 0.2) +
  geom_line(aes(color = disease), size = 1.05, stat = 'density', show.legend = F) +
  facet_grid(sex~urbanRural) +
  xlab('Age') +
  ylab('Density') +
  theme_eD() + # Custom theme
  scale_fill_manual(values = c('No Disease' = '#71C47D',
                               'Disease X' = '#EA817A')) +
  scale_color_manual(values = c('No Disease' = '#71C47D',
                                'Disease X' = '#EA817A')) +
  guides(colour = guide_legend(override.aes = list(color = c('#71C47D', '#EA817A'))))
```

Although I used a density plot in the example above, some may prefer using a box-plot as a suitable alternative. 

```{r include = FALSE}
plotData %>%
  mutate(urbanRural = factor(urbanRural, labels = c('Rural', 'Urban')),
         disease = factor(disease, labels = c('No Disease', 'Disease X'))) %>%
  
  ggplot() +
  geom_boxplot(aes(y = age, fill = disease),
               outlier.fill = NULL, 
               outlier.shape = 21, 
               width = 0.5) +
  facet_grid(vars(urbanRural, sex)) +
  xlab('') +
  ylab('Age') +
  coord_flip() +
  theme_eD() + # Custom theme
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  scale_fill_manual(values = c('No Disease' = '#71C47D',
                               'Disease X' = '#EA817A')) +
  scale_color_manual(values = c('No Disease' = '#71C47D',
                                'Disease X' = '#EA817A')) +
  guides(colour = guide_legend(override.aes = list(color = c('#71C47D', '#EA817A'))))
```

Age seems to have a strong impact on the disease outcome which may be masking more subtle relationships. When we remove age we observe urban areas have a higher proportion of *Disease X*, and there is subtle evidence of a higher proportion of disease among females. None of this should come as a surprise, as we set these relationships in advance but it is nice to validate them through these figures.
```{r, echo = FALSE, fig.cap = 'Proportion of <i>Disease X</i> in terms of sex and location.', out.width = "100%", out.extra = "style= 'margin-bottom: 0rem;'" }
# Disease by age, sex, urban
plotData  %>%
  count(disease, sex, urbanRural) %>% 
  left_join(count(plotData, sex, urbanRural, name = 'total'), by = c('sex', 'urbanRural')) %>%
  mutate(urbanRural = factor(urbanRural, labels = c('Rural', 'Urban')),
         disease = factor(disease, labels = c('No Disease', 'Disease X')),
         propDisease = 100 * n/total) %>%
  
  ggplot(aes(y = propDisease, x = sex)) +
  geom_bar(aes(fill = disease), stat = 'identity', color = 'transparent', alpha = 0.6) +
   facet_grid(~urbanRural) +
  xlab('Sex') +
  ylab('Proportion with Disease X') +
  theme_eD() + # Custom theme
  scale_fill_manual(values = c('No Disease' = '#71C47D',
                               'Disease X' = '#EA817A')) +
  scale_color_manual(values = c('No Disease' = '#71C47D',
                                'Disease X' = '#EA817A')) +
  guides(colour = guide_legend(override.aes = list(color = c('#71C47D', '#EA817A'))))
```

## A case for simulated data

You may be asking yourself, why would I bother 'making up' data, it seems like a lot of trouble? It even may appear to reinforce bias as the data is often a product from our own assumptions. However, in reality, simulated data have a great amount of value, here are just a few examples:

- Simulation allows us to know the exact data generation process, an important tool to troubleshoot statistical models and see where they succeed or fail.
- Creating 'What-If' scenarios can provide valuable insight for program and policy planning.
- Although there are many [publicly-available datasets](#resources) in our digital age, many are highly aggregated, difficult to link to other important sources, and hard to access when content is highly sensitive (e.g. health records)

Focusing on the last point, it can be very difficult to access and share interesting data in public health. For example, most sources that share infectious disease information have limited content to reduce the chance of identifying individuals. Sometimes there isn't even enough information available to do basic descriptive epidemiology, which usually requires key demographics (e.g. age, sex), locations (e.g. urban/rural, health authority), and time (e.g. diagnosis date, treatment administration date). Obviously, this can raise concerns about privacy. For meaningful and interesting analyses this type of information is needed and to obtain access a data-sharing agreement and ethical approval is often required. For academic research endeavors this is a necessity but for this website I resort to real data where it is openly available and simulated data when it is not. When the focus is to showcase specific methods, simulated data is all we really need. Even common problems that plague 'real' data, such as missing information, can be added to our simulated data.

### Simulation toolbox

Agreeing on the usefulness of simulating data, how can it be done? There are a many approaches with varying complexity:

- Agent/individual-based models 
- Random variables simulated from probability distributions
- Creating Bayesian networks based upon priors and/or learned from data

Most approaches are available in R or its host of packages, albeit with a tradeoff between flexibility and complexity. For instance, [NetLogo](https://ccl.northwestern.edu/netlogo/) is a programming environment for agent-based simulation and can connect to R through the *RNetLogo* package; this however still requires knowledge of the NetLogo syntax. Other simulation approaches in R are available from [HydeNet](https://cran.r-project.org/web/packages/HydeNet/index.html), [greta](https://greta-stats.org), [RStan](https://mc-stan.org/users/interfaces/rstan), and [simstudy](https://www.rdatagen.net/page/simstudy/). Some packages are limited as they only allow certain data types (e.g. discrete or continuous) and the possible relationships that can exist between them (e.g. linear trend). Typically, 'low-level' interfaces (i.e. a package that gets you close to the 'engine') allow more freedom but increase the learning curve and chances of blunder. Furthermore, a selection of methods listed may some amount of data to generate the models and simulate additional samples; for some purposes, this may be unsuitable.

I am partial to Bayesian networks, which is my method of choice in this post, as I find their associated diagrams a powerful visualization tool to understand the model. They can also be created from our imagination, or trained on any available data we happen to have. Flexible and powerful!

### Basic simulation example 

Before I continue to explain the method I used to create Bayes' Town, now is a good time to demonstrate that simulation techniques are not be out of anyone's reach. 

Keeping with the theme of health, let's simulate the probability of death by boredom in Bayes' Town. Hopefully you are not suffering from this affliction as it is quite serious, as we will soon discover.

As a binary outcome, we can use the **binomial** distribution for modeling death. The probability of death will be a function of an intercept term and the binary explanatory variable for 'boredom' (0: not bored, 1: bored). The **logit link** will map the linear combination of the intercept and 'boredom' term between 0% and 100% probability. In math jargon this looks like:

\begin{align*}
& y_i \sim Binomial(n, p_i) \\
& logit(p_i) = \alpha + \beta x_i\\
\end{align*}

<img src="https://media.giphy.com/media/3o6Ztf2G1n4a5QQlWw/giphy.gif"/>

If you are like me, you may get nervous when a page becomes overtaken by formulae. I promise, it is not as bad as it looks. I usually feel more at ease once I see the formulae translated to code, which we will do now.

```{r}
# Set 100,000 people in our population
n <- 1e5

# Half of our population is bored
bored <- rep(c(0,1), each = n/2)

# Bored has a small increase to the log-odds of death
beta <- 0.3

# The baseline chance of death is low
alpha <- 0.05

# The log-odds of the outcome (arithmetic from the formula above!)
logodds <- alpha + beta * bored

# Probability of death (size = 1, only one death possible per person!)
death_probability <- rbinom(n, prob = plogis(logodds), size = 1)

# Final dataset
data <- data.frame(boredom = bored,
                   death = death_probability)

```

We can see a relationship between boredom and death in a simple 2x2 table. The values on the bottom of the table compare the odds of not being bored among the living and dead; these results show a 1.4x higher odds of death among those experiencing boredom. 
```{r}
library(knitr)
library(kableExtra)

# Function to calculate odds
odds <- function(x) round(x[[1]]/x[[2]], 3)

# Create table with odds, adjust rounding, and format
summary_tbl <- table(Boredom = data$boredom,
                     Death = data$death) %>% 
  addmargins(margin = c(1,2),
             FUN = list(Odds = odds),
             quiet = T)

summary_tbl[c(1:3), c(1,3)] <- as.character(summary_tbl[c(1:3), c(1,3)])

summary_tbl %>%
  kable(caption = 'Relationship between boredom and death.') %>%
  kable_styling(full_width = T) %>%
  pack_rows('Boredom', 1,2, label_row_css = 'border-bottom:0px solid;') %>%
  add_header_above(c(" ", "Death" = 3), align = 'l') %>%
  scroll_box(box_css = "", extra_css = 'overflow-x: auto; width: 100%')
```


If that isn't enough to convince you that the simulated data created a positive correlation between boredom and death, let's pass the data through a logisitic regression model to see how well we can retrieve the original parameters for the intercept (0.05) and beta term (0.3). 

```{r}
coef(glm(death ~ boredom, data = data, family = 'binomial')) %>%
  round(3) %>%
  kable(col.names = 'Estimate', caption = 'Parameter estimates for model on boredom and death.') %>%
  kable_styling() %>%
  scroll_box(box_css = "",
             extra_css = 'overflow-x: auto; width: 50%')
```

That is pretty close! We could take this simulation further and create a dependency of boredom upon additional terms. I encourage you to explore other situations, such as collinear variables. As the simulation becomes increasingly complex other approaches may be easier to maintain. This is what we will discuss next.

## Data simulation with Bayesian networks

Another approach to simulate data is using BNs, specifically **hybrid** BNs which are more complex but allow the use of both continuous and discrete variables. Although raw data are typically used included in these models, it is also possible to create BNs based solely on our own conceptions. In this case, we can say we are creating a network using **prior probability distributions**.

Up until now we have focused on the outputs from the data simulation in hopes of showcasing its capabilities without yet being cowed in either boredom or fear from maths. For those willing, understanding the details will curb the feeling of mathematical mysticism and avoid ascribing too much faith in the simulation and underlying model. Although there are many strategies to simulate data, in this example I have used a Bayesian Network (BN), known by several other names, including the less exciting, directed graphical model. The overall idea is simple:

1. First create a relationship using a graph, consisting of nodes and arcs. Each node represents a random variable (e.g. age, sex) and their arcs define relationships. Mathematically, this is typically seen as: $G = (X, A)$, where $X$ is a set of variables. 
1. Next, we assign probability distributions to the variables. BNs are useful as they can describe the *global* probabilistic relationship among $X$ by their various *local* distributions. That is to say, we understand the probability of every node by the relationship it has to its direct parent nodes. Again, as Greek soup this is seen as: $Pr\mathbf(X) = \prod_{i=1}^{p}Pr(X{_i}|\textstyle\prod_{X_i})$.

When we know both the graph as well as the local distribution, the model is called an *expert system*. In other words, everything is defined  upfront by us; we aren't using any data to directly  fit the model. In Bayesian lingo, we are essentially making a model with just priors. 

<img src="https://media.giphy.com/media/ToMjGpNzwVayAyJQteU/giphy.gif"/>

If you feel overwhelmed do not fret, I felt this way initially too (and perhaps still do). Deeper understanding comes at a cost. Fortunately, half of our job is already done as we have previously defined the [graph](#htmlwidget-1) using our infinite wisdom. Now we just need to decide on the probability distributions and how they relate between each node. 

### Model definition

Similar to our simple example, we will start by assigning our **parameter priors**, essentially a set of distributions for the intercept and coefficient terms in the model.

$$
\begin{align*}
& \textbf{Prior parameters}\\
& a_{disease} \sim Normal(-10, 10) \\
& b_{age} \sim Normal(0.015, 10) \\
& b_{sex} \sim Normal(0.005, 10) \\
& b_{urbanity} \sim Normal(5, 10) \\ \\ 
\end{align*}
$$
$a_{disease}$ is the baseline odds of disease and the other parameters define the magnitude by which they increase the odds of disease. Next, we create distributions for our variables of interest (alternatively, these could be set as fixed values).

$$
\begin{align*}
& \textbf{Random variable priors}\\
& sex \sim  Binom(propMale, 1) \\ 
& urbanRural \sim Binom(0.8, 1) \\
& \begin{aligned} age \sim Gamma((age_{shape} + &age\_param * sex),\\ 1/age_{scale}) \end{aligned}\\ \\
\end{align*}
$$

We also will have several input parameters for our variables. We can adjust these or the prior parameters to create strong/weaker, or entirely different, relationships.

$$
\begin{align*}
& \textbf{Input parameters}\\
& propMale = 0.52  \\ 
& age_{shape} = 6 \\
& age_{scale} = 6 \\
& age\_param = 0.7 \\ \\
\end{align*}
$$

And finally, we need to assign a likelihood function for the prior predictive simulation. This is the distribution for *Disease X* based upon the relationship of age, sex, and location.

$$
\begin{align*}
& \textbf{Likelihood function}\\
& disease \sim Binom(p, 1) \\
& \begin{aligned} logit(p) &= a_{disease} \\
&+ b_{age}(age) \\
&+ b_{sex}(sex) \\
&+ b_{urbanity}(urbanRural)\\\\ \end{aligned} 
\end{align*}
$$

These formulae may look confusing, but they are outlining very similar concepts to the simple example between boredom and death. If you are a little lost, the DAG from the beginning should help. In plain English we are saying that:

- Sex is slightly more likely to be male
- Location has a much higher chance to be urban
- Age is slightly dependent upon sex; females reach slightly older age groups on average
- *Disease X* is a linear relationship between age, sex, and location
- The strength of age, sex, and location are pulled from a distribution of values that we can tweak to increase/decrease their strength.

In R code it looks quite similar: 

```{r, eval = F}
# Set input parameters (could be included directly in the model below if we arent changing often)
inputParam <- list(p_sex = 0.52, 
                   age_shape = 6,
                   age_scale = 6,
                   age_param = 0.7)

# Define model (character string is code for JAGS)
simModel <-  as.character('
                      model {
                        a_disease ~ dnorm(-10, 10);
                        b_age ~ dnorm(.015, 10);
                        b_sex ~ dnorm(.005, 10);
                        b_urbanity ~ dnorm(5, 10);
                      
                        sex ~ dbinom(p_sex, 1);
                        urbanRural ~ dbinom(0.8, 1);
                        age ~ dgamma((age_shape + age_param * sex), 1/age_scale);
                      
                        logit(p) <- a_disease + b_age * age + b_sex * sex + b_urbanity * urbanRural;
                        disease ~ dbinom(p, 1);
                      }')
```

This is the set of rules that governs Bayes' Town citizens. Right now it provides a single snapshot but it is possible to extend this to a *dynamic* BN to create multiple time slices. 

**Bayes' Theorem** is used to solve this type of question and the equation may look simple enough... $$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$ I do not provide details here as there are a plethora of resources online that go through Bayes' Theorem; however, I will point out that a special 'engine' is often required to solve these models. When the distributions are complex, the solutions become non-trivial. Whilst some examples such as the beta-binomial have closed-form solutions, others require special sampling methods to solve, which we will discuss next.


### MCMC and `rjags`

With the rules now defined to create *Disease X* in Bayes' Town, we now need to perform the simulation. The procedure used is called **markov chain monte carlo (MCMC)**, a stochastic method to estimate posterior probability distributions. In short this means we collect samples of possible parameter values instead of attempting to directly compute them. The results are then understood in terms of their sample distribution. However, generating these samples is not always easy and there are various MCMC methods to tackle this challenge, albeit with their own limitations and takeoffs.  

<strong>1. Metropolis algorithm: </strong> original and 'simple' MCMC implementation.  
<strong>2. Metropolis-Hastings algorithm:</strong> an extension to the metropolis algorithm.  
<strong>3. Gibbs sampling:</strong> more efficient sampling by using *adaptive proposals* (`BUGS` and `JAGS` software use this method).  
<strong>4. Hamiltonian Monte Carlo (HMC):</strong> runs a (resource expensive) physics simulation to generate informative samples. HMC provides useful diagnostics but currently cannot sample discrete parameters (not easily at least). `STAN` uses this method and is considered harder to learn than `BUGS`.  

Numerous `R` packages implement the assortment of MCMC methods but require, or are built on top of, an `R` interface to `STAN` (e.g. `rstan`) or `JAGS` (e.g. `rjags`). For my current purposes, I decided to use `rjags` (authored by *Martyn Plummer* who is also the creator of `JAGS`) since it can generate discrete values and follows a [syntax](http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf) similar to `R`. Using `rjags` directly is a bit more complex (as we saw in the [model definitions](#model-definition)) but provides flexibility and customization. 

There are three steps to run a model with `JAGS`: (1) Define, (2) Compile, and (3) Simulate. We have already done the first step, for the rest I created helper functions to pass the model to `JAGS` and retrieve the samples! This may be unnecessary but I found it to be a helpful abstraction. We can always dip into `rjags` if we need something more specific. 

The `R6` package was used to encapsulate the `BayesTown` object created from the simulation. This object is created by running `BayesTown$new()` with the provided model (`simModel`) and input parameters (`inputParam`). `JAGS` will then compile and perform the simulation, returning the results to the assigned object class. A subset of variables can be retained via the `set_variables()` method and a new draw can be done via `resample()`. Various diagnostics are also accessible. If this is still a bit unclear, the complete example in the next section may be of help. 

## Full simulation run

At the beginning a model and parameters were already determined. Below I provide the code to create the simulated *Bayes' Town*. A few helper functions built on top of `rjags` make the process a bit more friendly to read. The *Bayes' Town* example is an expert system using just priors but if we decide to include data in the model we simply add a loop to the likelihood function.

```{r example, results = FALSE}
# Set input parameters
inputParam <- list(p_sex = 0.52, 
                   age_shape = 6,
                   age_scale = 6,
                   age_param = 0.7)

# Define model
simModel <-  as.character('
                      model {
                        a_disease ~ dnorm(-10, 10);
                        b_age ~ dnorm(.015, 10);
                        b_sex ~ dnorm(.005, 10);
                        b_urbanity ~ dnorm(5, 10);
                      
                        sex ~ dbinom(p_sex, 1);
                        urbanRural ~ dbinom(0.8, 1);
                        age ~ dgamma((age_shape + age_param * sex), 1/age_scale);
                      
                        logit(p) <- a_disease + b_age * age + b_sex * sex + b_urbanity * urbanRural;
                        disease ~ dbinom(p, 1);
                      }')

# Simulate new town
newTown <- BayesTown$new(population_size = 1000,
                      jags_data = inputParam, 
                      n.chains = 2,
                      n.iter = 5000,
                      jags_model = simModel)

# Keep only subset of parameters of interest
newTown$set_variables(c('age', 'sex', 'urbanRural', 'disease'), mode = 'include')

# Resample data
newTown$resample()
```

We can now use `newTown` as we saw at the start of the post and explore our newly simulated dataset!           

## Resources
### Useful data sources
1. [Kaggle](https://www.kaggle.com/tags/healthcare)
1. [WHO](https://apps.who.int/gho/data/node.resources)
1. [US Gov](https://www.data.gov/)
1. [CDC](https://wonder.cdc.gov/Welcome.html) 
1. [NHS data catalogue ](https://data.england.nhs.uk/datase)
1. [Alberta Open Datasets](http://www.ahw.gov.ab.ca/IHDA_Retrieval/) 
1. [Canada Open Data](https://open.canada.ca/en/open-data)  
1. [UCI Machine Learning Repository](https://perma.cc/M3XC-M9HU)

### Software and details on simulation methods
1. [NetLogo](https://ccl.northwestern.edu/netlogo/) and [RNetLogo](https://rdrr.io/cran/RNetLogo/man/RNetLogo-package.html)
1. [Paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0208775) that uses NetLogo for ABM of infectious disease.
1. [simstudy](https://www.rdatagen.net/page/simstudy/) 
1. [epimodel](https://www.epimodel.org/), mathematical epidemiology models in R
1. [HydeNet](https://cran.r-project.org/web/packages/HydeNet/index.html)
1. [bnlearn](https://www.bnlearn.com/ Bayesian simluation)
1. [bayesvl](https://github.com/sshpa/bayesvl)
1. [Blog](https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/) that introduces data simulation with probability distributions

### Information on Bayesian statistics
While there are many blogs and online articles about Bayesian statistics and Bayesian networks; I found some actually books were my best resource; both are enjoyable and worth the price tag. 

1. [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) and related [YouTube channel](https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/featured) 
1. [Bayesian Networks with Examples in R](https://www.routledge.com/Bayesian-Networks-With-Examples-in-R/Scutari-Denis/p/book/9781482225587)
1. [Bayesian Networks without Tears](https://www.aaai.org/ojs/index.php/aimagazine/article/view/918), a great introduction to the topic.
